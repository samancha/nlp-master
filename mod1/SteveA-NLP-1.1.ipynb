{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### To install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!pip install nltk \n",
        "!python3 -m spacy download en_core_web_sm\n",
        "\n",
        "# Download dataset to root of the project with this file in the same directory "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-11 16:29:49.406155: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from nltk.stem import PorterStemmer\n",
        "from spacy import displacy\n",
        "import os\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Main custom function for data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPRIVklG9Gzy",
        "outputId": "cd52b421-f7da-4f70-960a-dd8e57bcf9ef"
      },
      "outputs": [],
      "source": [
        "def process_document(sentence, remove_stopwords=True):    \n",
        "    doc = nlp(sentence)\n",
        "    doc_tokens = doc\n",
        "\n",
        "    # Removing stop words from doc\n",
        "    if remove_stopwords:\n",
        "        doc_tokens = [token for token in doc if not token.is_stop]\n",
        "    \n",
        "    # Tokenization\n",
        "    tokens = [token.text for token in doc_tokens]\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"Number of tokens:\", len(tokens))\n",
        "\n",
        "    # Initialize stemmer and Stemming of tokens\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(word) for word in tokens]\n",
        "    print(\"Stems:\", stems)\n",
        "    print(\"Number of Stems:\", len(stems))\n",
        "\n",
        "    # Lemmas\n",
        "    lemmas = [token.lemma_ for token in doc_tokens]\n",
        "    print(\"Lemmas:\", lemmas)\n",
        "    print(\"Number of Lemmas:\", len(lemmas))\n",
        "\n",
        "    # Stop words\n",
        "    stop_words = [token.text for token in doc_tokens if token.is_stop]\n",
        "    print(\"Checking for Stop Words:\", stop_words)\n",
        "\n",
        "    word_freq = Counter()\n",
        "\n",
        "    # Counting the frequency of words\n",
        "    for token in doc_tokens:\n",
        "        if not token.is_punct and not token.is_stop:\n",
        "            word_freq[token.text.lower()] += 1  # Convert to lowercase\n",
        "\n",
        "    # Get the most common words\n",
        "    common_words = word_freq.most_common(5)\n",
        "    print(\"Most common words:\", common_words)\n",
        "\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results of sample file aclImdb/test/neg/0_2.txt\n",
        "Raw movie review \n",
        "tokens, stems, lemmas all equealed 187 tokens\n",
        "\n",
        "After stop words removed \n",
        "tokens, stems, and lemmas = 79 tokens\n",
        "\n",
        "Most common words with counter: \n",
        "[('costner', 4), ('care', 3), ('kutcher', 3), ('ghosts', 2), ('closet', 2)]\n",
        "\n",
        "An image of the results produced by the name entity visualizer can be view in Figure 1 of the report. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Raw\n",
            "\n",
            "Tokens: ['Once', 'again', 'Mr.', 'Costner', 'has', 'dragged', 'out', 'a', 'movie', 'for', 'far', 'longer', 'than', 'necessary', '.', 'Aside', 'from', 'the', 'terrific', 'sea', 'rescue', 'sequences', ',', 'of', 'which', 'there', 'are', 'very', 'few', 'I', 'just', 'did', 'not', 'care', 'about', 'any', 'of', 'the', 'characters', '.', 'Most', 'of', 'us', 'have', 'ghosts', 'in', 'the', 'closet', ',', 'and', 'Costner', \"'s\", 'character', 'are', 'realized', 'early', 'on', ',', 'and', 'then', 'forgotten', 'until', 'much', 'later', ',', 'by', 'which', 'time', 'I', 'did', 'not', 'care', '.', 'The', 'character', 'we', 'should', 'really', 'care', 'about', 'is', 'a', 'very', 'cocky', ',', 'overconfident', 'Ashton', 'Kutcher', '.', 'The', 'problem', 'is', 'he', 'comes', 'off', 'as', 'kid', 'who', 'thinks', 'he', \"'s\", 'better', 'than', 'anyone', 'else', 'around', 'him', 'and', 'shows', 'no', 'signs', 'of', 'a', 'cluttered', 'closet', '.', 'His', 'only', 'obstacle', 'appears', 'to', 'be', 'winning', 'over', 'Costner', '.', 'Finally', 'when', 'we', 'are', 'well', 'past', 'the', 'half', 'way', 'point', 'of', 'this', 'stinker', ',', 'Costner', 'tells', 'us', 'all', 'about', 'Kutcher', \"'s\", 'ghosts', '.', 'We', 'are', 'told', 'why', 'Kutcher', 'is', 'driven', 'to', 'be', 'the', 'best', 'with', 'no', 'prior', 'inkling', 'or', 'foreshadowing', '.', 'No', 'magic', 'here', ',', 'it', 'was', 'all', 'I', 'could', 'do', 'to', 'keep', 'from', 'turning', 'it', 'off', 'an', 'hour', 'in', '.']\n",
            "Number of tokens: 187\n",
            "Stems: ['onc', 'again', 'mr.', 'costner', 'ha', 'drag', 'out', 'a', 'movi', 'for', 'far', 'longer', 'than', 'necessari', '.', 'asid', 'from', 'the', 'terrif', 'sea', 'rescu', 'sequenc', ',', 'of', 'which', 'there', 'are', 'veri', 'few', 'i', 'just', 'did', 'not', 'care', 'about', 'ani', 'of', 'the', 'charact', '.', 'most', 'of', 'us', 'have', 'ghost', 'in', 'the', 'closet', ',', 'and', 'costner', \"'s\", 'charact', 'are', 'realiz', 'earli', 'on', ',', 'and', 'then', 'forgotten', 'until', 'much', 'later', ',', 'by', 'which', 'time', 'i', 'did', 'not', 'care', '.', 'the', 'charact', 'we', 'should', 'realli', 'care', 'about', 'is', 'a', 'veri', 'cocki', ',', 'overconfid', 'ashton', 'kutcher', '.', 'the', 'problem', 'is', 'he', 'come', 'off', 'as', 'kid', 'who', 'think', 'he', \"'s\", 'better', 'than', 'anyon', 'els', 'around', 'him', 'and', 'show', 'no', 'sign', 'of', 'a', 'clutter', 'closet', '.', 'hi', 'onli', 'obstacl', 'appear', 'to', 'be', 'win', 'over', 'costner', '.', 'final', 'when', 'we', 'are', 'well', 'past', 'the', 'half', 'way', 'point', 'of', 'thi', 'stinker', ',', 'costner', 'tell', 'us', 'all', 'about', 'kutcher', \"'s\", 'ghost', '.', 'we', 'are', 'told', 'whi', 'kutcher', 'is', 'driven', 'to', 'be', 'the', 'best', 'with', 'no', 'prior', 'inkl', 'or', 'foreshadow', '.', 'no', 'magic', 'here', ',', 'it', 'wa', 'all', 'i', 'could', 'do', 'to', 'keep', 'from', 'turn', 'it', 'off', 'an', 'hour', 'in', '.']\n",
            "Number of Stems: 187\n",
            "Lemmas: ['once', 'again', 'Mr.', 'Costner', 'have', 'drag', 'out', 'a', 'movie', 'for', 'far', 'long', 'than', 'necessary', '.', 'aside', 'from', 'the', 'terrific', 'sea', 'rescue', 'sequence', ',', 'of', 'which', 'there', 'be', 'very', 'few', 'I', 'just', 'do', 'not', 'care', 'about', 'any', 'of', 'the', 'character', '.', 'Most', 'of', 'we', 'have', 'ghost', 'in', 'the', 'closet', ',', 'and', 'Costner', \"'s\", 'character', 'be', 'realize', 'early', 'on', ',', 'and', 'then', 'forget', 'until', 'much', 'later', ',', 'by', 'which', 'time', 'I', 'do', 'not', 'care', '.', 'the', 'character', 'we', 'should', 'really', 'care', 'about', 'be', 'a', 'very', 'cocky', ',', 'overconfident', 'Ashton', 'Kutcher', '.', 'the', 'problem', 'be', 'he', 'come', 'off', 'as', 'kid', 'who', 'think', 'he', 'be', 'well', 'than', 'anyone', 'else', 'around', 'he', 'and', 'show', 'no', 'sign', 'of', 'a', 'cluttered', 'closet', '.', 'his', 'only', 'obstacle', 'appear', 'to', 'be', 'win', 'over', 'Costner', '.', 'finally', 'when', 'we', 'be', 'well', 'past', 'the', 'half', 'way', 'point', 'of', 'this', 'stinker', ',', 'Costner', 'tell', 'we', 'all', 'about', 'Kutcher', \"'s\", 'ghost', '.', 'we', 'be', 'tell', 'why', 'Kutcher', 'be', 'drive', 'to', 'be', 'the', 'good', 'with', 'no', 'prior', 'inkling', 'or', 'foreshadowing', '.', 'no', 'magic', 'here', ',', 'it', 'be', 'all', 'I', 'could', 'do', 'to', 'keep', 'from', 'turn', 'it', 'off', 'an', 'hour', 'in', '.']\n",
            "Number of Lemmas: 187\n",
            "Checking for Stop Words: ['Once', 'again', 'has', 'out', 'a', 'for', 'than', 'from', 'the', 'of', 'which', 'there', 'are', 'very', 'few', 'I', 'just', 'did', 'not', 'about', 'any', 'of', 'the', 'Most', 'of', 'us', 'have', 'in', 'the', 'and', \"'s\", 'are', 'on', 'and', 'then', 'until', 'much', 'by', 'which', 'I', 'did', 'not', 'The', 'we', 'should', 'really', 'about', 'is', 'a', 'very', 'The', 'is', 'he', 'off', 'as', 'who', 'he', \"'s\", 'than', 'anyone', 'else', 'around', 'him', 'and', 'no', 'of', 'a', 'His', 'only', 'to', 'be', 'over', 'when', 'we', 'are', 'well', 'the', 'of', 'this', 'us', 'all', 'about', \"'s\", 'We', 'are', 'why', 'is', 'to', 'be', 'the', 'with', 'no', 'or', 'No', 'here', 'it', 'was', 'all', 'I', 'could', 'do', 'to', 'keep', 'from', 'it', 'off', 'an', 'in']\n",
            "Most common words: [('costner', 4), ('care', 3), ('kutcher', 3), ('ghosts', 2), ('closet', 2)]\n",
            "\n",
            "\n",
            "\n",
            "Data With stop words removed\n",
            "\n",
            "Tokens: ['Mr.', 'Costner', 'dragged', 'movie', 'far', 'longer', 'necessary', '.', 'Aside', 'terrific', 'sea', 'rescue', 'sequences', ',', 'care', 'characters', '.', 'ghosts', 'closet', ',', 'Costner', 'character', 'realized', 'early', ',', 'forgotten', 'later', ',', 'time', 'care', '.', 'character', 'care', 'cocky', ',', 'overconfident', 'Ashton', 'Kutcher', '.', 'problem', 'comes', 'kid', 'thinks', 'better', 'shows', 'signs', 'cluttered', 'closet', '.', 'obstacle', 'appears', 'winning', 'Costner', '.', 'Finally', 'past', 'half', 'way', 'point', 'stinker', ',', 'Costner', 'tells', 'Kutcher', 'ghosts', '.', 'told', 'Kutcher', 'driven', 'best', 'prior', 'inkling', 'foreshadowing', '.', 'magic', ',', 'turning', 'hour', '.']\n",
            "Number of tokens: 79\n",
            "Stems: ['mr.', 'costner', 'drag', 'movi', 'far', 'longer', 'necessari', '.', 'asid', 'terrif', 'sea', 'rescu', 'sequenc', ',', 'care', 'charact', '.', 'ghost', 'closet', ',', 'costner', 'charact', 'realiz', 'earli', ',', 'forgotten', 'later', ',', 'time', 'care', '.', 'charact', 'care', 'cocki', ',', 'overconfid', 'ashton', 'kutcher', '.', 'problem', 'come', 'kid', 'think', 'better', 'show', 'sign', 'clutter', 'closet', '.', 'obstacl', 'appear', 'win', 'costner', '.', 'final', 'past', 'half', 'way', 'point', 'stinker', ',', 'costner', 'tell', 'kutcher', 'ghost', '.', 'told', 'kutcher', 'driven', 'best', 'prior', 'inkl', 'foreshadow', '.', 'magic', ',', 'turn', 'hour', '.']\n",
            "Number of Stems: 79\n",
            "Lemmas: ['Mr.', 'Costner', 'drag', 'movie', 'far', 'long', 'necessary', '.', 'aside', 'terrific', 'sea', 'rescue', 'sequence', ',', 'care', 'character', '.', 'ghost', 'closet', ',', 'Costner', 'character', 'realize', 'early', ',', 'forget', 'later', ',', 'time', 'care', '.', 'character', 'care', 'cocky', ',', 'overconfident', 'Ashton', 'Kutcher', '.', 'problem', 'come', 'kid', 'think', 'well', 'show', 'sign', 'cluttered', 'closet', '.', 'obstacle', 'appear', 'win', 'Costner', '.', 'finally', 'past', 'half', 'way', 'point', 'stinker', ',', 'Costner', 'tell', 'Kutcher', 'ghost', '.', 'tell', 'Kutcher', 'drive', 'good', 'prior', 'inkling', 'foreshadowing', '.', 'magic', ',', 'turn', 'hour', '.']\n",
            "Number of Lemmas: 79\n",
            "Checking for Stop Words: []\n",
            "Most common words: [('costner', 4), ('care', 3), ('kutcher', 3), ('ghosts', 2), ('closet', 2)]\n"
          ]
        }
      ],
      "source": [
        "# read file inside local folder test/neg/0_2.txt\n",
        "text = None\n",
        "svg = None\n",
        "file_path = '0_2.txt'\n",
        "with open('aclImdb/test/neg/0_2.txt', 'r') as file:\n",
        "    text = file.read().replace('\\n', '')\n",
        "\n",
        "    # process the data raw \n",
        "    print(\"Data Raw\\n\")\n",
        "    process_document(text, remove_stopwords=False)\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"Data With stop words removed\\n\")\n",
        "    # process the data with stop words removed\n",
        "    doc = process_document(text, remove_stopwords=True)\n",
        "    \n",
        "    svg = displacy.render(doc, style=\"ent\", minify=True,jupyter=False)\n",
        "    file_name = file_path.split(\".\")[0]\n",
        "    output_path = Path(\"img_res/test/neg/\" + file_name + \"_ent.svg\")\n",
        "    output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Code for running on a directory\n",
        "\n",
        "This cell works and was able to produce the most insight by seeing the various reviews with techniques used in the process_document function.  \n",
        "\n",
        "Ideally I'd want to make seperate output files for this data, the entity recognizer and the dependency visualizer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir_path = \"aclImdb/test/neg\"\n",
        "output_path = 'img_res/test/neg/'\n",
        "\n",
        "# Iterate directory\n",
        "for file_name in os.listdir(dir_path):\n",
        "    # Create the full file path by joining the directory path and filename\n",
        "    file_path = os.path.join(dir_path, file_name)\n",
        "\n",
        "    # Check if the current file_path is a file\n",
        "    if os.path.isfile(file_path):\n",
        "        # Read the text from the file\n",
        "        with open(file_path, 'r') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # process the data raw \n",
        "        print(\"Data Raw\\n\")\n",
        "        process_document(text, remove_stopwords=False)\n",
        "        print(\"\\n\\n\")\n",
        "        print(\"Data With stop words removed\\n\")\n",
        "        # process the data with stop words removed\n",
        "        doc = process_document(text, remove_stopwords=True)\n",
        "\n",
        "        # Works but commented out to show other stats on multiple txt files\n",
        "        # Creating the entity Recognizer \n",
        "        # svg = displacy.render(doc, style=\"ent\", minify=True,jupyter=False)\n",
        "        # file_name = file_path.split(\".\")[0]\n",
        "        # output_path = Path(\"img_res/test/neg/\" + file_name + \"_ent.svg\")\n",
        "        # output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating svg's of depenency parse\n",
        "The dependency visualizer, dep, shows part-of-speech tags and syntactic dependencies.   \n",
        "Creates a custom collered one with a compact view that is more square based shaped connections.   \n",
        "Images can be seen in Figure 2 of the report. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "res = []\n",
        "dir_path = \"aclImdb/test/neg\"\n",
        "output_path = 'img_res/test/neg/'\n",
        "\n",
        "# Iterate directory\n",
        "for file_name in os.listdir(dir_path):\n",
        "\n",
        "    file_path = os.path.join(dir_path, file_name)\n",
        "    # check if current file_path is a file \n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            text = f.read()\n",
        "        doc = nlp(text)\n",
        "        options = {\"compact\": True, \"bg\": \"#09a3d5\",\"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
        "        \n",
        "        # Creating the dependency visualizer\n",
        "        svg = displacy.render(doc, style=\"dep\",minify=True,jupyter=False, options=options)\n",
        "        file_name = file_path.split(\".\")[0]\n",
        "        output_path = Path(\"img_res/test/neg/\" + file_name + \"_dep.svg\")\n",
        "        output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Printing readable pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NbConvertApp] Converting notebook SteveA-NLP-1.1.ipynb to pdf\n",
            "/usr/local/lib/python3.11/site-packages/nbconvert/utils/pandoc.py:51: RuntimeWarning: You are using an unsupported version of pandoc (3.1.3).\n",
            "Your version must be at least (1.12.1) but less than (3.0.0).\n",
            "Refer to https://pandoc.org/installing.html.\n",
            "Continuing with doubts...\n",
            "  check_pandoc_version()\n",
            "[NbConvertApp] Writing 43236 bytes to notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 52482 bytes to SteveA-NLP-1.1.pdf\n"
          ]
        }
      ],
      "source": [
        "!jupyter nbconvert --to pdf SteveA-NLP-1.1.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git restore --staged <file>...\" to unstage)\n",
            "\t\u001b[32mmodified:   .gitignore\u001b[m\n",
            "\t\u001b[32mnew file:   Module_2_Lab_Session.ipynb\u001b[m\n",
            "\t\u001b[32mmodified:   SteveA-NLP-1.1.ipynb\u001b[m\n",
            "\t\u001b[32mnew file:   SteveA-NLP-1.1.pdf\u001b[m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
