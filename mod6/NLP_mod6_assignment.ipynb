{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in /usr/local/lib/python3.11/site-packages (0.6.2)\n",
      "Requirement already satisfied: keras-core in /usr/local/lib/python3.11/site-packages (from keras-nlp) (0.1.7)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from keras-nlp) (1.24.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from keras-nlp) (23.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/site-packages (from keras-nlp) (2023.8.8)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/site-packages (from keras-nlp) (13.6.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/site-packages (from keras-nlp) (0.1.8)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/site-packages (from keras-core->keras-nlp) (0.0.7)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/site-packages (from keras-core->keras-nlp) (3.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich->keras-nlp) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# install only once\n",
    "# ! pip install kaggle\n",
    "# ! pip install -q --upgrade keras-nlp\n",
    "\n",
    "\n",
    "# Download the dataset from kaggle\n",
    "# ! kaggle datasets download -d shashwatwork/consume-complaints-dataset-fo-nlp\n",
    "# unzip it\n",
    "# !tar -zxf consume-complaints-dataset-fo-nlp.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          BertTokenizer, TFBertForSequenceClassification, InputExample, InputFeatures)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras_nlp\n",
    "import keras_core as keras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "set_seed(58)\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovery of null values: \n",
      "\n",
      " Unnamed: 0    0\n",
      "product       0\n",
      "narrative     0\n",
      "dtype: int64\n",
      "\n",
      "Distribution of products: \n",
      "\n",
      " product\n",
      "credit_reporting       91172\n",
      "debt_collection        23148\n",
      "mortgages_and_loans    18990\n",
      "credit_card            15566\n",
      "retail_banking         13535\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique product values: \n",
      " ['credit_card', 'retail_banking', 'credit_reporting', 'mortgages_and_loans', 'debt_collection']\n",
      "\n",
      "Narative sentence word count describe:\n",
      "  count    162411.000000\n",
      "mean         80.232798\n",
      "std         108.872213\n",
      "min           1.000000\n",
      "25%          27.000000\n",
      "50%          50.000000\n",
      "75%          95.000000\n",
      "max        2685.000000\n",
      "Name: narrative, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('complaints_processed.csv', index_col=False)\n",
    "# remove rows with null values\n",
    "df = df.dropna()\n",
    "print(\"Discovery of null values: \\n\\n\", df.isnull().sum())\n",
    "print(\"\\nDistribution of products: \\n\\n\", df['product'].value_counts())\n",
    "print(\"\\nUnique product values: \\n\", df['product'].unique().tolist())\n",
    "print(\"\\nNarative sentence word count describe:\\n \", df['narrative'].apply(lambda x: len(str(x).split())).describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>narrative</th>\n",
       "      <th>num_words</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>retail_banking</th>\n",
       "      <th>credit_reporting</th>\n",
       "      <th>mortgages_and_loans</th>\n",
       "      <th>debt_collection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>credit_card</td>\n",
       "      <td>purchase order day shipping amount receive pro...</td>\n",
       "      <td>230</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>credit_card</td>\n",
       "      <td>forwarded message date tue subject please inve...</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>retail_banking</td>\n",
       "      <td>forwarded message cc sent friday pdt subject f...</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>credit_reporting</td>\n",
       "      <td>payment history missing credit report speciali...</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>credit_reporting</td>\n",
       "      <td>payment history missing credit report made mis...</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            product                                          narrative   \n",
       "0       credit_card  purchase order day shipping amount receive pro...  \\\n",
       "1       credit_card  forwarded message date tue subject please inve...   \n",
       "2    retail_banking  forwarded message cc sent friday pdt subject f...   \n",
       "3  credit_reporting  payment history missing credit report speciali...   \n",
       "4  credit_reporting  payment history missing credit report made mis...   \n",
       "\n",
       "   num_words  credit_card  retail_banking  credit_reporting   \n",
       "0        230            1               0                 0  \\\n",
       "1        132            1               0                 0   \n",
       "2        173            0               1                 0   \n",
       "3        131            0               0                 1   \n",
       "4        123            0               0                 1   \n",
       "\n",
       "   mortgages_and_loans  debt_collection  \n",
       "0                    0                0  \n",
       "1                    0                0  \n",
       "2                    0                0  \n",
       "3                    0                0  \n",
       "4                    0                0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "# calculate the number of words in each narrative\n",
    "# Notice that type in each row didn't defautl to string, so i had to convert to string prior to splitting\n",
    "df['num_words'] = df['narrative'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "product_labels = df['product'].unique().tolist()\n",
    "\n",
    "for label in product_labels:\n",
    "    df[label] = df['product'].apply(lambda x: 1 if x == label else 0)\n",
    "\n",
    "# create columns \n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['please reference complaint retailed well fargo filing complaint agency please read email attached received well fargo today requesting information already provided asking house vacant instructed vacate premise well fargo counter offer house listed year ago countering offer settle bottom email said correspondence particular loan guideline outlined specific loan interpreted well fargo home mortgage loan blatant retaliation filing complaint cfpb asking assistance senator assistance email senator office attached well fargo stop',\n",
       "       'authorized user purchased jewelry cost user receive final product never received final product made dispute citi found merchant favor wrote letter citi appeal heard back willing take merchant court citi employee witness',\n",
       "       'short summary information account showing report still inaccurate sent letter acknowledging inaccurate changed account open past due year charged activity adversely impacted credit score payment history moved bad credit rating good credit tier reporting day past due amount due open status charged year account issue reported cfpb cfpb complaint number attached response received would speak due represented lawyer response sent demanding information regarding inaccurate information reported across three credit bureau attached demand information letter xxxxpdf replied via certified mail addressing information reported inaccurately across bureau verified accurate several time prior updated data provider updated would indicate change made change made could accurate first response instead replied following information included response based review account reported account unpaid charge balance record indicate charged vehicle associated account sold auction attached letterxxxxpdf received alert experian credit score dropped account updated derogatory information account reporting account information inaccurately even response received stating account charged report reflects inaccurately account open day past due payment due monthly attached see one day prior information showing closed attached company able report information across three bureau accurately reporting manner fraudulent damaging credit profile transacted done business described information reported credit report cfpb reviewed fulfilled demand first request information would deleted credit profile would impacted ability qualify new credit term rate right economic downturn combat veteran late payment month sudden company report credit missed straight payment month straight information need completely deleted credit report demand compensation amount bureau information reported compensated demand per occurrence additional per day per occurrence day remains report adversely impact ability finance negotiate favorable term based actual credit worthiness due incompetence negligence company deletion immediate payment mailed address last correspondence addition experian accountable well reported activity bureau previously action prevent occurring',\n",
       "       'purchased first home nj using mortgage loan wherein latter designated lender principal balance approximately upon putting approximately around discovered foregoing transaction discharged well fargo home mortgage wfhm without knowledge consent upon someone creating refinance dated without knowledge consent wfhm continued taking monthly payment continued taking payment towards believed original purchase knowing said payment going towards forged refinance furthermore received proper credit payment made month today foreclosing home new jersey docket since year present date using forged refinance mortgage loan recorded year see exhibit expert handwriting expert confirmed way report dated someone authored forged name mortgage loan document displayed within exhibit b annexed hereto furthermore commenced foreclosure annexed hereto exhibit c assignment instrument asserting transfer forged mortgage loan dated display email communication conspiring create document see exhibit',\n",
       "       'beginning year decided refinance home combination credit score income documentation lead u finding dba mortgage banker year due diligence folk upfront sent page banking statement reviewed detail ensure qualified financially starting loan process took u week navigate loan qualification process another week closing end result deed trust recorded home never given loan reached stewart title day deed recorded learn intention funding begun process attempting rescind deed trust made decision halt funding attempt rescind loan without letting u know collusion two company reason neither made effort update u monday tuesday morning conscious decision made withhold information attempt rescind documentation without communicating u week silence received email telling u sent stewart title satisfaction deed trust satisfaction loan never loan misfiled deed trust remained home week communication timeframe still never received anything writing happened qualified closed loan chose back hour colluded stewart title normal title company rescind deed trust without telling u supply company page personal document also paid appraisal meant nothing could offer product justify result current climate left financial ruin answer two corporation walked u company treated u poorly throughout process especially time fulfill side qualified transaction nc bar association reviewed information along page timeline event full communication occurred real property division decided launch investigation ongoing time litigation real estate attorney also reviewed information stated valid claim noted breach stewart title duty u client refusing disburse fund well breach loan commitment identified nc statute good fund settlement act item violated transaction bottom line multiple violation contained page rest information assist understanding timeline event negligence part stewart title',\n",
       "       'sending payment shellpoint mortgage directly bank bank show delivered reference number shellpoint posting account keep getting collection call credit dropped point sent proof payment get response every time call get circular message leave number call back get response sent posting stopping automatic payment get previous one posted right abuse character credit score',\n",
       "       'sent convergent multiple letter requesting validation made pursuant fair debt collection practice act fair credit reporting act along corresponding local state law please note requesting validation competent evidence bearing signature showing ever contractual obligation pay convergent sent billing statement showing balance credit report show inaccurate information therefore requesting validation debt bearing signature',\n",
       "       'may concern writing dispute fraudulent charge account amount victim identity theft make authorize charge requesting charge removed finance charge related fraudulent amount credited well receive accurate statement request made pursuant fair credit billing act amendment truth lending act see also b writing request method verification dispute initiated subsequent response received enclosed letter accordance fcra section requesting information review completeness accuracy appropriateness lieu sending information reopen dispute ensure proper investigation performed would appreciate timely response outlining step occur resolve matter receive response choice exercise right frca section pursue legal action closed account acct opened balance account auto account acct opened balance account',\n",
       "       'decided get credit pulled due bill received mail never heard place immediately suspected identity theft reviewed report caught surprise couple derogatory item never opened life thing listed application mine please remove unknown item report contacted ftc local police behalf please remove report company inquiry',\n",
       "       'transferred student loan time monthly payment never received information regarding transfer found transfer emergency declaration related flooding contacted ask deferment granted time tell change account noticed since still monthly payment month reported day late payment three credit bureau applied deferment period emergency declaration remove late payment notification credit report never paid money today owe money date incorrectly reported account delinquent even though owe money tried contacting refuse change reporting reported delinquent even though emergency declaration made communication'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create traing and test sets\n",
    "X = df['narrative'].values\n",
    "y = df[['credit_card', 'retail_banking', 'credit_reporting', 'mortgages_and_loans', 'debt_collection']].values\n",
    "\n",
    "# Convert X to a NumPy array of shape (n_samples, 1) to match y's shape of (n_samples, n_labels)\n",
    "# y.shape (2224, 5)\n",
    "X = np.array(X).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=58, stratify=y)\n",
    "\n",
    "# Reshape to reshape nparrays from 2d to 1d for vectorization\n",
    "X_train = X_train.reshape(-1)\n",
    "X_test = X_test.reshape(-1)\n",
    "\n",
    "# create 5 rows of data to test the model\n",
    "test_data = X_test[:10]\n",
    "test_labels = y_test[:10]\n",
    "display(test_data)\n",
    "display(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tokenization and conversion of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a max_length that suits your data\n",
    "max_length = 512  \n",
    "batch_size = 64\n",
    "labels = [0,1]\n",
    "labels_dict = {'credit_card': 0, 'retail_banking': 1, 'credit_reporting': 2, 'mortgages_and_loans': 3, 'debt_collection': 4}  \n",
    "\n",
    "def labels_to_int(labels, label_dict):\n",
    "    \"\"\" Convert label to int.\n",
    "    Returns: List of converted labels.\n",
    "    \"\"\"\n",
    "    # Convert labels to integers\n",
    "    label_ids = [label_dict[label] for label in labels]\n",
    "    return label_ids\n",
    "\n",
    "def index_labels(labels, label_dict):\n",
    "    \"\"\" Convert label to int.\n",
    "    Returns: List of converted labels.\n",
    "    \"\"\"\n",
    "    # Convert labels to integers\n",
    "    label_ids = [label_dict[label] for label in labels]\n",
    "    return label_ids\n",
    "# int_labels = labels_to_int(test_labels, labels_dict)\n",
    "# print(int_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputExample(guid='0', text_a='please reference complaint retailed well fargo filing complaint agency please read email attached received well fargo today requesting information already provided asking house vacant instructed vacate premise well fargo counter offer house listed year ago countering offer settle bottom email said correspondence particular loan guideline outlined specific loan interpreted well fargo home mortgage loan blatant retaliation filing complaint cfpb asking assistance senator assistance email senator office attached well fargo stop', text_b=None, label=array([0, 0, 0, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "# Convert texts to InputExamples\n",
    "train_examples = [InputExample(guid=str(i), text_a=test_data[i], label=test_labels[i]) for i in range(len(test_data))]\n",
    "\n",
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train feature:  InputFeatures(input_ids=[101, 3531, 4431, 12087, 7027, 2098, 2092, 23054, 15242, 12087, 4034, 3531, 3191, 10373, 4987, 2363, 2092, 23054, 2651, 17942, 2592, 2525, 3024, 4851, 2160, 10030, 10290, 12436, 16280, 18458, 2092, 23054, 4675, 3749, 2160, 3205, 2095, 3283, 4675, 2075, 3749, 7392, 3953, 10373, 2056, 11061, 3327, 5414, 5009, 4179, 14801, 3563, 5414, 10009, 2092, 23054, 2188, 14344, 5414, 1038, 20051, 4630, 18695, 15242, 12087, 12935, 2361, 2497, 4851, 5375, 5205, 5375, 10373, 5205, 2436, 4987, 2092, 23054, 2644, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=array([0, 0, 0, 1, 0]))\n",
      "input ids:  [101, 3531, 4431, 12087, 7027, 2098, 2092, 23054, 15242, 12087, 4034, 3531, 3191, 10373, 4987, 2363, 2092, 23054, 2651, 17942, 2592, 2525, 3024, 4851, 2160, 10030, 10290, 12436, 16280, 18458, 2092, 23054, 4675, 3749, 2160, 3205, 2095, 3283, 4675, 2075, 3749, 7392, 3953, 10373, 2056, 11061, 3327, 5414, 5009, 4179, 14801, 3563, 5414, 10009, 2092, 23054, 2188, 14344, 5414, 1038, 20051, 4630, 18695, 15242, 12087, 12935, 2361, 2497, 4851, 5375, 5205, 5375, 10373, 5205, 2436, 4987, 2092, 23054, 2644, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert InputExamples to InputFeatures\n",
    "def convert_examples_to_features(examples, tokenizer, max_length=max_length, label_list=labels, output_mode=\"classification\"):\n",
    "    features = []\n",
    "    for example in examples:\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            example.text_a,\n",
    "            add_special_tokens=True, \n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True,\n",
    "            max_length=max_length, \n",
    "        )\n",
    "        \n",
    "\n",
    "        features.append(InputFeatures(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], label=example.label))\n",
    "    return features\n",
    "\n",
    "train_features = convert_examples_to_features(train_examples, tokenizer, max_length=max_length, label_list=test_labels)\n",
    "print(\"train feature: \",train_features[0])\n",
    "print(\"input ids: \",train_features[0].input_ids)\n",
    "print(\"attention mask: \",train_features[0].attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.from_tensor_slices_op._TensorSliceDataset'>\n",
      "<_TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(512,), dtype=tf.int32, name=None)}, TensorSpec(shape=(5,), dtype=tf.int64, name=None))>\n",
      "<class 'tensorflow.python.data.ops.batch_op._BatchDataset'>\n",
      "<_BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 512), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Convert InputFeatures to TensorFlow datasets\n",
    "def convert_features_to_tf_dataset(features):\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_labels = []\n",
    "\n",
    "    for feature in features:\n",
    "        all_input_ids.append(feature.input_ids)\n",
    "        all_attention_masks.append(feature.attention_mask)\n",
    "        all_labels.append(feature.label)\n",
    "\n",
    "    # Convert lists to TF tensors\n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices(({\"input_ids\": all_input_ids, \"attention_mask\": all_attention_masks}, all_labels))\n",
    "    return tf_ds\n",
    "\n",
    "train_dataset = convert_features_to_tf_dataset(train_features)\n",
    "print(type(train_dataset))\n",
    "print(train_dataset)\n",
    "\n",
    "batch_dataset = train_dataset.shuffle(3).batch(3)\n",
    "print(type(batch_dataset))\n",
    "print(batch_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "ids: tf.Tensor(\n",
      "[[  101  2460 12654 ...     0     0     0]\n",
      " [  101  9362  5310 ...     0     0     0]\n",
      " [  101  4156  2034 ...     0     0     0]], shape=(3, 512), dtype=int32)\n",
      "mask: tf.Tensor(\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]], shape=(3, 512), dtype=int32)\n",
      "labels: tf.Tensor(\n",
      "[[0 0 1 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 1]], shape=(3, 5), dtype=int64)\n",
      "ids: tf.Tensor(\n",
      "[[  101  3531  4431 ...     0     0     0]\n",
      " [  101  2741 28314 ...     0     0     0]\n",
      " [  101  6016  7909 ...     0     0     0]], shape=(3, 512), dtype=int32)\n",
      "mask: tf.Tensor(\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]], shape=(3, 512), dtype=int32)\n",
      "labels: tf.Tensor(\n",
      "[[0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]], shape=(3, 5), dtype=int64)\n",
      "ids: tf.Tensor(\n",
      "[[ 101 2089 5142 ...    0    0    0]\n",
      " [ 101 2927 2095 ...    0    0    0]\n",
      " [ 101 4015 3076 ...    0    0    0]], shape=(3, 512), dtype=int32)\n",
      "mask: tf.Tensor(\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]], shape=(3, 512), dtype=int32)\n",
      "labels: tf.Tensor(\n",
      "[[0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]], shape=(3, 5), dtype=int64)\n",
      "ids: tf.Tensor(\n",
      "[[  101  2787  2131  4923  2766  2349  3021  2363  5653  2196  2657  2173\n",
      "   3202  6878  4767 11933  8182  3189  3236  4474  3232  4315 18170  7062\n",
      "   8875  2196  2441  2166  2518  3205  4646  3067  3531  6366  4242  8875\n",
      "   3189 11925  3027  2278  2334  2610  6852  3531  6366  3189  2194  9934\n",
      "    102     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]], shape=(1, 512), dtype=int32)\n",
      "mask: tf.Tensor(\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]], shape=(1, 512), dtype=int32)\n",
      "labels: tf.Tensor([[0 0 1 0 0]], shape=(1, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the batched dataset\n",
    "print(len(batch_dataset))\n",
    "for batch in batch_dataset:\n",
    "    input_ids = batch[0][\"input_ids\"]\n",
    "    attention_mask = batch[0][\"attention_mask\"]\n",
    "    print(\"ids:\", input_ids)\n",
    "    print(\"mask:\", attention_mask)\n",
    "    print(\"labels:\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "64\n",
      "2030\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_RepeatDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Steve/dev/aiMasters/NLP/mod6/NLP_mod6_assignment.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Steve/dev/aiMasters/NLP/mod6/NLP_mod6_assignment.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Steve/dev/aiMasters/NLP/mod6/NLP_mod6_assignment.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m val \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39mskip(\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Steve/dev/aiMasters/NLP/mod6/NLP_mod6_assignment.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(train_dataset[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39minput_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Steve/dev/aiMasters/NLP/mod6/NLP_mod6_assignment.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(val)\n",
      "\u001b[0;31mTypeError\u001b[0m: '_RepeatDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "split = 0.8 \n",
    "print(batch_size)\n",
    "size = int((len(X) / batch_size) * split)\n",
    "print(size)\n",
    "\n",
    "train = train_dataset.take(1)\n",
    "val = train_dataset.skip(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition, training, and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and intstantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-06, clipnorm=1.0),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy('accuracy')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1672, in train_step\n        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 5) and (None, 2) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/Steve/dev/aiMasters/NLP/mod6/NLP_mod6_assignment.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Steve/dev/aiMasters/NLP/mod6/NLP_mod6_assignment.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(batch_dataset , epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/zr/tvl66dbj4756z6y3p0_gfv680000gn/T/__autograph_generated_file49xhjh5h.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1672\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1669\u001b[0m             y_pred \u001b[39m=\u001b[39m y_pred[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1671\u001b[0m     \u001b[39mif\u001b[39;00m loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1672\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompiled_loss(y, y_pred, sample_weight, regularization_losses\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlosses)\n\u001b[1;32m   1674\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m   1675\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mminimize(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable_variables, tape\u001b[39m=\u001b[39mtape)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1672, in train_step\n        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 5) and (None, 2) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model.fit(batch_dataset , epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer parameters\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "    # print(classification_report(y_test,nb.predict(X_test)))\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"I think this is amazing!\"\n",
    "test_input = tf_tokenizer.encode_plus(test_sentence, add_special_tokens=True, return_tensors=\"tf\")\n",
    "outputs = model(test_input[\"input_ids\"], attention_mask=test_input[\"attention_mask\"])\n",
    "probs = tf.nn.softmax(outputs[0], axis=-1)\n",
    "predicted_label = tf.argmax(probs, axis=1).numpy()[0]\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
